{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02541da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<torch.cuda.device object at 0x00000257932BF610>\n",
      "1\n",
      "GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97f098",
   "metadata": {},
   "source": [
    "# Dataset Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c05219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mean and standard deviation of pixel values to be used for image normalization.\n",
    "\"\"\"\n",
    "mean = [0.2773, 0.2589, 0.2684]\n",
    "std = [0.1650, 0.1639, 0.1658]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Transformations for training datasets are randomly augmented with \n",
    "horizontal flips, rotations and brightness adjustments.\n",
    "\n",
    "Images in test datasets are preserved as-is.\n",
    "\"\"\"\n",
    "train_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                      transforms.Resize((224, 224)),\n",
    "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                      transforms.RandomRotation(degrees=30),\n",
    "                                      transforms.ColorJitter(brightness=0.2),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean,std)\n",
    "                                      ])\n",
    "\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                     transforms.Resize((224, 224)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean,std)\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a78d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChimpNamesDataset(Dataset):\n",
    "    IMG_SIZE = 224\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.name_labels = self.annotations['name'].values\n",
    "        \n",
    "        #Encoding name strings into unique integers to be used as labels.\n",
    "        self.name_set = sorted(set(self.name_labels))\n",
    "        self.name_dict = {}\n",
    "        for i in range(len(self.name_set)):\n",
    "            self.name_dict[self.name_set[i]] = i\n",
    "            \n",
    "        self.annotations['labels'] = self.annotations.name.map(self.name_dict)\n",
    "        self.annotations.head()\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = cv2.imread(img_path)\n",
    "        name_label = self.annotations.iloc[index, 10]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return (image, name_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652356e4",
   "metadata": {},
   "source": [
    "# Finding mean and std for Normalization\n",
    "\n",
    "By combining the train_1.csv and test_1.csv, a full sample of the database can be loaded for optimum means and standard deviations for image normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ebc8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = ChimpNamesDataset(csv_file='train_1.csv', root_dir='chimp_faces/', transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f0c738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = DataLoader(dataset,\n",
    "#                          batch_size=32,\n",
    "#                          num_workers=0,\n",
    "#                          shuffle=False)\n",
    "\n",
    "# mean = 0.\n",
    "# std = 0.\n",
    "# for images, _ in loader:\n",
    "#     batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "#     images = images.view(batch_samples, images.size(1), -1)\n",
    "#     mean += images.mean(2).sum(0)\n",
    "#     std += images.std(2).sum(0)\n",
    "\n",
    "# mean /= len(loader.dataset)\n",
    "# std /= len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a02f3b3",
   "metadata": {},
   "source": [
    "# Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef966072",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loaders = []\n",
    "test_loaders = []\n",
    "\n",
    "ds_train1 = ChimpNamesDataset(csv_file='train_1.csv', root_dir='chimp_faces/', transform=train_transform)\n",
    "ds_train2 = ChimpNamesDataset(csv_file='train_2.csv', root_dir='chimp_faces/', transform=train_transform)\n",
    "ds_train3 = ChimpNamesDataset(csv_file='train_3.csv', root_dir='chimp_faces/', transform=train_transform)\n",
    "ds_train4 = ChimpNamesDataset(csv_file='train_4.csv', root_dir='chimp_faces/', transform=train_transform)\n",
    "ds_train5 = ChimpNamesDataset(csv_file='train_5.csv', root_dir='chimp_faces/', transform=train_transform)\n",
    "\n",
    "ds_test1 = ChimpNamesDataset(csv_file='test_1.csv', root_dir='chimp_faces/', transform=train_transform)\n",
    "ds_test2 = ChimpNamesDataset(csv_file='test_2.csv', root_dir='chimp_faces/', transform=train_transform)\n",
    "ds_test3 = ChimpNamesDataset(csv_file='test_3.csv', root_dir='chimp_faces/', transform=train_transform)\n",
    "ds_test4 = ChimpNamesDataset(csv_file='test_4.csv', root_dir='chimp_faces/', transform=train_transform)\n",
    "ds_test5 = ChimpNamesDataset(csv_file='test_5.csv', root_dir='chimp_faces/', transform=train_transform)\n",
    "\n",
    "train1_loader = DataLoader(dataset=ds_train1, batch_size=batch_size, shuffle=True)\n",
    "train2_loader = DataLoader(dataset=ds_train2, batch_size=batch_size, shuffle=True)\n",
    "train3_loader = DataLoader(dataset=ds_train3, batch_size=batch_size, shuffle=True)\n",
    "train4_loader = DataLoader(dataset=ds_train4, batch_size=batch_size, shuffle=True)\n",
    "train5_loader = DataLoader(dataset=ds_train5, batch_size=batch_size, shuffle=True)\n",
    "train_loaders.append(train1_loader)\n",
    "train_loaders.append(train2_loader)\n",
    "train_loaders.append(train3_loader)\n",
    "train_loaders.append(train4_loader)\n",
    "train_loaders.append(train5_loader)\n",
    "\n",
    "test1_loader = DataLoader(dataset=ds_test1, batch_size=batch_size, shuffle=True)\n",
    "test2_loader = DataLoader(dataset=ds_test2, batch_size=batch_size, shuffle=True)\n",
    "test3_loader = DataLoader(dataset=ds_test3, batch_size=batch_size, shuffle=True)\n",
    "test4_loader = DataLoader(dataset=ds_test4, batch_size=batch_size, shuffle=True)\n",
    "test5_loader = DataLoader(dataset=ds_test5, batch_size=batch_size, shuffle=True)\n",
    "test_loaders.append(test1_loader)\n",
    "test_loaders.append(test2_loader)\n",
    "test_loaders.append(test3_loader)\n",
    "test_loaders.append(test4_loader)\n",
    "test_loaders.append(test5_loader)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b3c2d",
   "metadata": {},
   "source": [
    "# Training and Visualizing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ee06d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy against a provided test dataset.\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(\n",
    "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples):.2f}\"\n",
    "        )\n",
    "\n",
    "    model.train()\n",
    "    return float(num_correct)/float(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ec22a",
   "metadata": {},
   "source": [
    "# ResNet18 (ImageNet) Transfer Learning with Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d7e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_finetune(epochs, num_classes, train_dataloaders, test_dataloaders, model_name, subfolder_name):\n",
    "    \"\"\"\n",
    "    Train a ResNet18 model, including the imported pretrained weights from ImageNet.\n",
    "    \"\"\"\n",
    "    main_save_folder = 'saved_models/'\n",
    "    full_save_directory = main_save_folder+subfolder_name+'/'\n",
    "    accuracy_list = []\n",
    "    #Loop for all five splits\n",
    "    for i in range(5):\n",
    "        numbered_name = ''\n",
    "        print(\"\\nSplit\", str(i+1) + \":\")\n",
    "        numbered_name = model_name + '-' + str(i+1)+ '.pth'\n",
    "        full_save_path = full_save_directory + numbered_name\n",
    "        #Load pretrained ResNet18 Model\n",
    "        model_ft = models.resnet18(pretrained=True)\n",
    "        model_ft  = model_ft.cuda() if device else model_ft\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        #Add classification layers\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adagrad(model_ft.parameters(), lr=0.001)\n",
    "        step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "        #activate train mode for model\n",
    "        model_ft.train()\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            for batch_idx, (data, targets) in enumerate(train_dataloaders[i]):\n",
    "                # Get data to cuda if possible\n",
    "                data = data.to(device=device)\n",
    "                targets = targets.to(device=device)\n",
    "\n",
    "                # forward\n",
    "                scores = model_ft(data)\n",
    "                loss = criterion(scores, targets)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # gradient descent or adam step\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch} Average Loss: {sum(losses)/len(losses):.5f}\")\n",
    "\n",
    "            if epoch%5==0:\n",
    "                print(f\"Epoch {epoch} Training Accuracy:\")\n",
    "                check_accuracy(train_dataloaders[i], model_ft)\n",
    "            step_lr_scheduler.step()\n",
    "        print(\"\\nEvaluation against test data:\")\n",
    "        accuracy = check_accuracy(test_dataloaders[i], model_ft)\n",
    "        accuracy_list.append(accuracy)\n",
    "        torch.save(model_ft.state_dict(), full_save_path)\n",
    "    print(\"\\n\\nAverage Accuracy: \", (sum(accuracy_list)/len(accuracy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4f3e5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split 1:\n",
      "Epoch 0 Average Loss: 2.63875\n",
      "Epoch 0 Training Accuracy:\n",
      "Got 2661 / 5155 with accuracy 0.52\n",
      "Epoch 1 Average Loss: 1.57734\n",
      "Epoch 2 Average Loss: 1.17944\n",
      "Epoch 3 Average Loss: 0.92689\n",
      "Epoch 4 Average Loss: 0.74597\n",
      "Epoch 5 Average Loss: 0.63173\n",
      "Epoch 5 Training Accuracy:\n",
      "Got 4715 / 5155 with accuracy 0.91\n",
      "Epoch 6 Average Loss: 0.52400\n",
      "Epoch 7 Average Loss: 0.43577\n",
      "Epoch 8 Average Loss: 0.43255\n",
      "Epoch 9 Average Loss: 0.41960\n",
      "\n",
      "Evaluation against test data:\n",
      "Got 1101 / 1331 with accuracy 0.83\n",
      "\n",
      "Split 2:\n",
      "Epoch 0 Average Loss: 2.61400\n",
      "Epoch 0 Training Accuracy:\n",
      "Got 2887 / 5155 with accuracy 0.56\n",
      "Epoch 1 Average Loss: 1.53815\n",
      "Epoch 2 Average Loss: 1.14497\n",
      "Epoch 3 Average Loss: 0.89706\n",
      "Epoch 4 Average Loss: 0.72329\n",
      "Epoch 5 Average Loss: 0.60995\n",
      "Epoch 5 Training Accuracy:\n",
      "Got 4688 / 5155 with accuracy 0.91\n",
      "Epoch 6 Average Loss: 0.50521\n",
      "Epoch 7 Average Loss: 0.42694\n",
      "Epoch 8 Average Loss: 0.41397\n",
      "Epoch 9 Average Loss: 0.40954\n",
      "\n",
      "Evaluation against test data:\n",
      "Got 1092 / 1331 with accuracy 0.82\n",
      "\n",
      "Split 3:\n",
      "Epoch 0 Average Loss: 2.67789\n",
      "Epoch 0 Training Accuracy:\n",
      "Got 2843 / 5155 with accuracy 0.55\n",
      "Epoch 1 Average Loss: 1.58936\n",
      "Epoch 2 Average Loss: 1.17248\n",
      "Epoch 3 Average Loss: 0.92137\n",
      "Epoch 4 Average Loss: 0.76623\n",
      "Epoch 5 Average Loss: 0.62092\n",
      "Epoch 5 Training Accuracy:\n",
      "Got 4674 / 5155 with accuracy 0.91\n",
      "Epoch 6 Average Loss: 0.53149\n",
      "Epoch 7 Average Loss: 0.45094\n",
      "Epoch 8 Average Loss: 0.43482\n",
      "Epoch 9 Average Loss: 0.42166\n",
      "\n",
      "Evaluation against test data:\n",
      "Got 1067 / 1331 with accuracy 0.80\n",
      "\n",
      "Split 4:\n",
      "Epoch 0 Average Loss: 2.62940\n",
      "Epoch 0 Training Accuracy:\n",
      "Got 2670 / 5155 with accuracy 0.52\n",
      "Epoch 1 Average Loss: 1.58156\n",
      "Epoch 2 Average Loss: 1.18417\n",
      "Epoch 3 Average Loss: 0.92727\n",
      "Epoch 4 Average Loss: 0.76594\n",
      "Epoch 5 Average Loss: 0.63723\n",
      "Epoch 5 Training Accuracy:\n",
      "Got 4676 / 5155 with accuracy 0.91\n",
      "Epoch 6 Average Loss: 0.52653\n",
      "Epoch 7 Average Loss: 0.45790\n",
      "Epoch 8 Average Loss: 0.43319\n",
      "Epoch 9 Average Loss: 0.42335\n",
      "\n",
      "Evaluation against test data:\n",
      "Got 1085 / 1331 with accuracy 0.82\n",
      "\n",
      "Split 5:\n",
      "Epoch 0 Average Loss: 2.64113\n",
      "Epoch 0 Training Accuracy:\n",
      "Got 2769 / 5155 with accuracy 0.54\n",
      "Epoch 1 Average Loss: 1.56987\n",
      "Epoch 2 Average Loss: 1.17020\n",
      "Epoch 3 Average Loss: 0.93220\n",
      "Epoch 4 Average Loss: 0.75656\n",
      "Epoch 5 Average Loss: 0.63794\n",
      "Epoch 5 Training Accuracy:\n",
      "Got 4660 / 5155 with accuracy 0.90\n",
      "Epoch 6 Average Loss: 0.53456\n",
      "Epoch 7 Average Loss: 0.44066\n",
      "Epoch 8 Average Loss: 0.43316\n",
      "Epoch 9 Average Loss: 0.42431\n",
      "\n",
      "Evaluation against test data:\n",
      "Got 1079 / 1331 with accuracy 0.81\n",
      "\n",
      "\n",
      "Average Accuracy:  0.8150262960180316\n"
     ]
    }
   ],
   "source": [
    "train_finetune(10, 86, train_loaders, test_loaders, 'resnet18-nofreeze', 'nofreeze')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668ff29",
   "metadata": {},
   "source": [
    "# ResNet18 (ImageNet) Transfer Learning with no Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "160c269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_freeze_params(epochs, num_classes, train_dataloaders, test_dataloaders, model_name, subfolder_name):\n",
    "    \"\"\"\n",
    "    Train a ResNet18 model without making changes to the imported pretrained weights from ImageNet.\n",
    "    Freeze all parameters from being trained apart from custom classification layers.\n",
    "    \"\"\"\n",
    "    main_save_folder = 'saved_models/'\n",
    "    full_save_directory = main_save_folder+subfolder_name+'/'\n",
    "    accuracy_list = []\n",
    "    #Loop for all five splits\n",
    "    for i in range(5):\n",
    "        numbered_name = ''\n",
    "        print(\"\\nSplit\", str(i+1) + \":\")\n",
    "        numbered_name = model_name + '-' + str(i+1)+ '.pth'\n",
    "        full_save_path = full_save_directory + numbered_name\n",
    "        #Load pretrained ResNet18 Model\n",
    "        model_freeze = models.resnet18(pretrained=True)\n",
    "        for param in model_freeze.parameters():\n",
    "            param.requires_grad = False\n",
    "        model_freeze  = model_freeze.cuda() if device else model_freeze\n",
    "        num_ftrs = model_freeze.fc.in_features\n",
    "        #Add classification layers\n",
    "        model_freeze.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        model_freeze = model_freeze.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adagrad(model_freeze.parameters(), lr=0.001)\n",
    "        step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "        #activate train mode for model\n",
    "        model_freeze.train()\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            for batch_idx, (data, targets) in enumerate(train_dataloaders[i]):\n",
    "                # Get data to cuda if possible\n",
    "                data = data.to(device=device)\n",
    "                targets = targets.to(device=device)\n",
    "\n",
    "                # forward\n",
    "                scores = model_freeze(data)\n",
    "                loss = criterion(scores, targets)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # gradient descent or adam step\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch} Average Loss: {sum(losses)/len(losses):.5f}\")\n",
    "\n",
    "            if epoch%5==0:\n",
    "                print(f\"Epoch {epoch} Training Accuracy:\")\n",
    "                check_accuracy(train_dataloaders[i], model_freeze)\n",
    "            step_lr_scheduler.step()\n",
    "        print(\"\\nEvaluation against test data:\")\n",
    "        accuracy = check_accuracy(test_dataloaders[i], model_freeze)\n",
    "        accuracy_list.append(accuracy)\n",
    "        torch.save(model_freeze.state_dict(), full_save_path)\n",
    "    print(\"\\n\\nAverage Accuracy: \", (sum(accuracy_list)/len(accuracy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e602f4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split 1:\n",
      "Epoch 0 Average Loss: 4.17883\n",
      "Epoch 0 Training Accuracy:\n",
      "Got 375 / 5155 with accuracy 0.07\n",
      "Epoch 1 Average Loss: 4.00423\n",
      "Epoch 2 Average Loss: 3.91313\n",
      "Epoch 3 Average Loss: 3.85037\n",
      "Epoch 4 Average Loss: 3.80619\n",
      "Epoch 5 Average Loss: 3.75023\n",
      "Epoch 5 Training Accuracy:\n",
      "Got 695 / 5155 with accuracy 0.13\n",
      "Epoch 6 Average Loss: 3.71597\n",
      "Epoch 7 Average Loss: 3.68375\n",
      "Epoch 8 Average Loss: 3.68260\n",
      "Epoch 9 Average Loss: 3.67541\n",
      "\n",
      "Evaluation against test data:\n",
      "Got 179 / 1331 with accuracy 0.13\n",
      "\n",
      "Split 2:\n",
      "Epoch 0 Average Loss: 4.20700\n",
      "Epoch 0 Training Accuracy:\n",
      "Got 375 / 5155 with accuracy 0.07\n",
      "Epoch 1 Average Loss: 4.03309\n",
      "Epoch 2 Average Loss: 3.95025\n",
      "Epoch 3 Average Loss: 3.87191\n",
      "Epoch 4 Average Loss: 3.81871\n",
      "Epoch 5 Average Loss: 3.77729\n",
      "Epoch 5 Training Accuracy:\n",
      "Got 650 / 5155 with accuracy 0.13\n",
      "Epoch 6 Average Loss: 3.73554\n",
      "Epoch 7 Average Loss: 3.71590\n",
      "Epoch 8 Average Loss: 3.70312\n",
      "Epoch 9 Average Loss: 3.69736\n",
      "\n",
      "Evaluation against test data:\n",
      "Got 161 / 1331 with accuracy 0.12\n",
      "\n",
      "Split 3:\n",
      "Epoch 0 Average Loss: 4.21114\n",
      "Epoch 0 Training Accuracy:\n",
      "Got 369 / 5155 with accuracy 0.07\n",
      "Epoch 1 Average Loss: 4.02171\n",
      "Epoch 2 Average Loss: 3.93319\n",
      "Epoch 3 Average Loss: 3.87629\n",
      "Epoch 4 Average Loss: 3.81609\n",
      "Epoch 5 Average Loss: 3.76615\n",
      "Epoch 5 Training Accuracy:\n",
      "Got 618 / 5155 with accuracy 0.12\n",
      "Epoch 6 Average Loss: 3.72377\n",
      "Epoch 7 Average Loss: 3.69870\n",
      "Epoch 8 Average Loss: 3.69963\n",
      "Epoch 9 Average Loss: 3.68810\n",
      "\n",
      "Evaluation against test data:\n",
      "Got 171 / 1331 with accuracy 0.13\n",
      "\n",
      "Split 4:\n",
      "Epoch 0 Average Loss: 4.21871\n",
      "Epoch 0 Training Accuracy:\n",
      "Got 340 / 5155 with accuracy 0.07\n",
      "Epoch 1 Average Loss: 4.03294\n",
      "Epoch 2 Average Loss: 3.95200\n",
      "Epoch 3 Average Loss: 3.87304\n",
      "Epoch 4 Average Loss: 3.81932\n",
      "Epoch 5 Average Loss: 3.77924\n",
      "Epoch 5 Training Accuracy:\n",
      "Got 714 / 5155 with accuracy 0.14\n",
      "Epoch 6 Average Loss: 3.73443\n",
      "Epoch 7 Average Loss: 3.70854\n",
      "Epoch 8 Average Loss: 3.69679\n",
      "Epoch 9 Average Loss: 3.69527\n",
      "\n",
      "Evaluation against test data:\n",
      "Got 164 / 1331 with accuracy 0.12\n",
      "\n",
      "Split 5:\n",
      "Epoch 0 Average Loss: 4.17854\n",
      "Epoch 0 Training Accuracy:\n",
      "Got 342 / 5155 with accuracy 0.07\n",
      "Epoch 1 Average Loss: 4.00818\n",
      "Epoch 2 Average Loss: 3.91517\n",
      "Epoch 3 Average Loss: 3.84946\n",
      "Epoch 4 Average Loss: 3.79501\n",
      "Epoch 5 Average Loss: 3.75501\n",
      "Epoch 5 Training Accuracy:\n",
      "Got 705 / 5155 with accuracy 0.14\n",
      "Epoch 6 Average Loss: 3.71888\n",
      "Epoch 7 Average Loss: 3.68711\n",
      "Epoch 8 Average Loss: 3.68860\n",
      "Epoch 9 Average Loss: 3.67732\n",
      "\n",
      "Evaluation against test data:\n",
      "Got 179 / 1331 with accuracy 0.13\n",
      "\n",
      "\n",
      "Average Accuracy:  0.12832456799398947\n"
     ]
    }
   ],
   "source": [
    "train_freeze_params(10, 86, train_loaders, test_loaders, 'resnet18-yesfreeze', 'yesfreeze')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87be31d",
   "metadata": {},
   "source": [
    "# Loading and Evaluating\n",
    "Please disable training code by commenting it out and run all the way from the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45ae3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model(load_path):\n",
    "#     loaded_model = models.resnet18(pretrained=True)\n",
    "#     loaded_model  = loaded_model.cuda() if device else loaded_model\n",
    "#     num_ftrs = loaded_model.fc.in_features\n",
    "#     #Add classification layers\n",
    "#     loaded_model.fc = nn.Linear(num_ftrs, 86)\n",
    "#     loaded_model = loaded_model.to(device)\n",
    "#     loaded_model.load_state_dict(torch.load(load_path))\n",
    "#     loaded_model.eval()\n",
    "#     for parameter in loaded_model.parameters():\n",
    "#         parameter.requires_grad = False\n",
    "\n",
    "#     loaded_model.eval()\n",
    "#     return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7160d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_list = []\n",
    "# for i in range(5):\n",
    "#     #reset model_name for next loop\n",
    "#     model_name = ''\n",
    "#     directory_name = 'saved_models/' + 'nofreeze' + '/'\n",
    "#     model_name = 'resnet18-nofreeze' + '-' + str(i+1) +'.pth'\n",
    "#     load_path = directory_name + model_name\n",
    "#     model = load_model(load_path)\n",
    "#     accuracy = check_accuracy(test_loaders[i], model)\n",
    "#     accuracy_list.append(accuracy)\n",
    "\n",
    "# print(\"\\n\\nAverage Accuracy: \", (sum(accuracy_list)/len(accuracy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a289b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
